{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"resnet.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"LaKyWMjynNJD","colab_type":"code","colab":{}},"source":["##RESTEST\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torchvision import transforms, datasets, models\n","\n","\n","USE_CUDA = torch.cuda.is_available()\n","DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n","\n","\n","# ## 하이퍼파라미터 \n","\n","EPOCHS     = 300\n","BATCH_SIZE = 128\n","\n","\n","# ## 데이터셋 불러오기\n","\n","train_loader = torch.utils.data.DataLoader(\n","    datasets.CIFAR10('./.data',\n","                   train=True,\n","                   download=True,\n","                   transform=transforms.Compose([\n","                       transforms.RandomCrop(32, padding=4),\n","                       transforms.RandomHorizontalFlip(),\n","                       transforms.ToTensor(),\n","                       transforms.Normalize((0.5, 0.5, 0.5),\n","                                            (0.5, 0.5, 0.5))])),\n","    batch_size=BATCH_SIZE, shuffle=True)\n","test_loader = torch.utils.data.DataLoader(\n","    datasets.CIFAR10('./.data',\n","                   train=False, \n","                   transform=transforms.Compose([\n","                       transforms.ToTensor(),\n","                       transforms.Normalize((0.5, 0.5, 0.5),\n","                                            (0.5, 0.5, 0.5))])),\n","    batch_size=BATCH_SIZE, shuffle=True)\n","\n","\n","# ## ResNet 모델 만들기\n","\n","class BasicBlock(nn.Module):\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3,\n","                               stride=stride, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n","                               stride=1, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, planes,\n","                          kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","    def __init__(self, num_classes=10):\n","        super(ResNet, self).__init__()\n","        self.in_planes = 16\n","\n","        self.conv1 = nn.Conv2d(3, 16, kernel_size=3,\n","                               stride=1, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(16) # 배치 정규화 활성화 함수 활성화 값 또는 출력값을 정규화하는 작업\n","        #신경망 각 레이어에서 데이터배치의 분포를 정규화하는 작업. 합성곱 수행 후, 배치 정규화?\n","        #각 히든 레이어에서 정규화를 하면서 입력 분포가 일정하게 되고, 이에 따른 learning rate를 크게 설정해도 괜찮아짐.\n","        # 학습 속도가 빨라진다. 결과적으로 conv2d 출력망에 맞춰서 작업\n","        self.layer1 = self._make_layer(16, 2, stride=1)\n","        self.layer2 = self._make_layer(32, 2, stride=2)\n","        self.layer3 = self._make_layer(64, 2, stride=2)\n","        self.linear = nn.Linear(64, num_classes)\n","\n","    def _make_layer(self, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(BasicBlock(self.in_planes, planes, stride))\n","            self.in_planes = planes\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.layer1(out)\n","        out = self.layer2(out)\n","        out = self.layer3(out)\n","        out = F.avg_pool2d(out, 8)\n","        out = out.view(out.size(0), -1)\n","        out = self.linear(out)\n","        return out\n","\n","# ## 준비\n","model = ResNet().to(DEVICE)\n","optimizer = optim.SGD(model.parameters(), lr=0.1,\n","                      momentum=0.9, weight_decay=0.0005)\n","scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n","\n","\n","# ## 학습\n","def train(model, train_loader, optimizer, epoch):\n","    model.train()\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        data, target = data.to(DEVICE), target.to(DEVICE)\n","        optimizer.zero_grad()\n","        output = model(data)\n","        loss = F.cross_entropy(output, target)\n","        loss.backward()\n","        optimizer.step()\n","\n","# ## 테스트\n","def evaluate(model, test_loader):\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    with torch.no_grad():\n","        for data, target in test_loader:\n","            data, target = data.to(DEVICE), target.to(DEVICE)\n","            output = model(data)\n","\n","            # 배치 오차를 합산\n","            test_loss += F.cross_entropy(output, target, reduction='sum').item()\n","\n","            # 가장 높은 값을 가진 인덱스가 바로 예측값\n","            pred = output.max(1, keepdim=True)[1]\n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","    test_loss /= len(test_loader.dataset)\n","    test_accuracy = 100. * correct / len(test_loader.dataset)\n","    return test_loss, test_accuracy\n","\n","for epoch in range(1, EPOCHS + 1):\n","    scheduler.step()\n","    train(model, train_loader, optimizer, epoch)\n","    test_loss, test_accuracy = evaluate(model, test_loader)\n","    \n","    print('[{}] Test Loss: {:.4f}, Accuracy: {:.2f}%'.format(\n","          epoch, test_loss, test_accuracy))"],"execution_count":0,"outputs":[]}]}